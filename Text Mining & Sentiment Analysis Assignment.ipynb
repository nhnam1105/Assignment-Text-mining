{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import glob\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading file\n",
    "art1 = \"\"\"Your Stemming 2020 letter letter 100 of the 7th was received night before last. I very\n",
    "cheerfully send you the twenty dollars, which sum you say is\n",
    "necessary to save your land from sale. It is singular that you\n",
    "should have forgotten a judgment against you; and it is more\n",
    "singular that the plaintiff should have let you forget it so long,\n",
    "particularly as I suppose you have always had property enough to\n",
    "satisfy a judgment of that amount. Before you pay it, it would be\n",
    "well to be sure you have not paid it; or, at least, that you can\n",
    "not prove you have paid it. Give my love to Mother, and all the\n",
    "connections.\"\"\"\n",
    "data_pos = pd.read_csv(\"/Volumes/DATA/aclImdb/pos.csv\", sep=';', error_bad_lines=False)\n",
    "data_pos.columns = [\"target\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Task 2\n",
    "\n",
    "#Split in sentences\n",
    "\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#print ('\\n-----\\n'.join(tokenizer.tokenize(a)))\n",
    "\n",
    "\n",
    "#Remove stopwords and punctuations\n",
    "def remove_stop_words(text):\n",
    "    stopwords_en = stopwords.words(\"english\")\n",
    "    stopwords_en = stopwords_en + list(string.punctuation)\n",
    "    cleaned_tokens = []\n",
    "    for w in nltk.word_tokenize(text):\n",
    "        w = w.lower()\n",
    "        if w not in stopwords_en:\n",
    "            cleaned_tokens.append(w)\n",
    "    cleaned_text = \" \".join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def split_sentence(article_text):\n",
    "    sentences = nltk.sent_tokenize(article_text)\n",
    "    return sentences\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def make_bigram(corpus):\n",
    "    tokens = tokenize(corpus)\n",
    "    bigram = nltk.ngrams(tokens, n = 2)\n",
    "    return bigram\n",
    "\n",
    "def generate_bow(document):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(document)\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(X.toarray())\n",
    "\n",
    "def tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform([art1])\n",
    "    tfidf = pd.DataFrame(X.toarray().transpose(), index=vectorizer.get_feature_names())\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 Create pipeline for implementing Task 1, part 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def _init_(self):\n",
    "        return\n",
    "    \n",
    "    #def _init_(self):\n",
    "        #return\n",
    "\n",
    "    def fit( self, X, y = None ):\n",
    "        return self\n",
    "\n",
    "    def clean (self, x):\n",
    "        cleaned_text = remove_stop_words(x)\n",
    "        tokens = tokenize(cleaned_text)\n",
    "        return tokens\n",
    "\n",
    "    def transform( self, X, y = None ):\n",
    "        return self.clean(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stem',\n",
       " 'letter',\n",
       " 'letter',\n",
       " '100',\n",
       " '7th',\n",
       " 'receiv',\n",
       " 'night',\n",
       " 'befor',\n",
       " 'last',\n",
       " 'veri',\n",
       " 'cheer',\n",
       " 'send',\n",
       " 'twenti',\n",
       " 'dollar',\n",
       " 'sum',\n",
       " 'say',\n",
       " 'necessari',\n",
       " 'save',\n",
       " 'land',\n",
       " 'sale',\n",
       " 'singular',\n",
       " 'forgotten',\n",
       " 'judgment',\n",
       " 'singular',\n",
       " 'plaintiff',\n",
       " 'let',\n",
       " 'forget',\n",
       " 'long',\n",
       " 'particular',\n",
       " 'suppos',\n",
       " 'alway',\n",
       " 'properti',\n",
       " 'enough',\n",
       " 'satisfi',\n",
       " 'judgment',\n",
       " 'amount',\n",
       " 'befor',\n",
       " 'pay',\n",
       " 'would',\n",
       " 'well',\n",
       " 'sure',\n",
       " 'paid',\n",
       " 'least',\n",
       " 'prove',\n",
       " 'paid',\n",
       " 'give',\n",
       " 'love',\n",
       " 'mother',\n",
       " 'connect']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipel_task1 = Pipeline([('clean', CleanText())])\n",
    "pipel_task1.transform(art1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I came at this film with high expectations. I was aware of Greenaway's work and 'The Tempest' and was interested in an adaptation. I first wanted to switch off after ten minutes, but felt that it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>POSSIBLE SPOILERSThe Spy Who Shagged Me is a muchly overrated and over-hyped sequel. International Man of Mystery came straight out of the blue. It was a lone star that few people had heard of. Bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>It has said that The Movies and Baseball both thrived during The Great Depression. It appears that the grim realities of a Nation caught up in the aftermath of this Economic Disaster created a nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Boring and appallingly acted(Summer Pheonix). She sounded more Asian than Jewish. Some of the scenes and costumes looked more mid 20th century than late 19th century. What on earth fine actors lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>I just came back from the Late-night cinema and it was indeed a silent way out as most of the audience pondered though the real-life black &amp; white images of the partition,the freedom movement,etc ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                                                                                                                                                                                  reviews\n",
       "0       0  I came at this film with high expectations. I was aware of Greenaway's work and 'The Tempest' and was interested in an adaptation. I first wanted to switch off after ten minutes, but felt that it ...\n",
       "1       0  POSSIBLE SPOILERSThe Spy Who Shagged Me is a muchly overrated and over-hyped sequel. International Man of Mystery came straight out of the blue. It was a lone star that few people had heard of. Bu...\n",
       "2       1  It has said that The Movies and Baseball both thrived during The Great Depression. It appears that the grim realities of a Nation caught up in the aftermath of this Economic Disaster created a nee...\n",
       "3       0  Boring and appallingly acted(Summer Pheonix). She sounded more Asian than Jewish. Some of the scenes and costumes looked more mid 20th century than late 19th century. What on earth fine actors lik...\n",
       "4       1  I just came back from the Late-night cinema and it was indeed a silent way out as most of the audience pondered though the real-life black & white images of the partition,the freedom movement,etc ..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Volumes/DATA/aclImdb/movie_reviews1.csv\", sep=';', error_bad_lines=False)\n",
    "data.columns = [\"target\", \"reviews\"]\n",
    "data['target'] = np.where(data['target']=='positive',1,0)\n",
    "pd.set_option('max_colwidth', 200)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data['reviews']\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing pipeline for Part B\n",
    "class PreProcess(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def _init_(self,):\n",
    "        return\n",
    "\n",
    "    def fit( self, X, y = None ):\n",
    "        return self\n",
    "\n",
    "    def clean (self, x):\n",
    "        cleaned_text = remove_stop_words(x)\n",
    "        #Remove number\n",
    "        cleaned_text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        return X.apply(self.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('pre_process',\n",
       "                 Pipeline(memory=None,\n",
       "                          steps=[('clean', PreProcess()),\n",
       "                                 ('tfidf',\n",
       "                                  TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                                  decode_error='strict',\n",
       "                                                  dtype=<class 'numpy.float64'>,\n",
       "                                                  encoding='utf-8',\n",
       "                                                  input='content',\n",
       "                                                  lowercase=True, max_df=1.0,\n",
       "                                                  max_features=None, min_df=1,\n",
       "                                                  ngram_range=(1, 1), norm='l2',\n",
       "                                                  preprocessor=None,\n",
       "                                                  smooth_idf=True,\n",
       "                                                  sto...\n",
       "                 SelectKBest(k=1000,\n",
       "                             score_func=<function chi2 at 0x7f8b4df13a70>)),\n",
       "                ('std_scaler',\n",
       "                 StandardScaler(copy=True, with_mean=False, with_std=True)),\n",
       "                ('classify',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=False,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=10000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.1, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe_clean = Pipeline([('clean', PreProcess()), ('tfidf', TfidfVectorizer())])\n",
    "\n",
    "pipe_svm = Pipeline([('pre_process', pipe_clean),\n",
    "                    ('select', SelectKBest(score_func = chi2, k = 1000)),\n",
    "                     (\"std_scaler\", StandardScaler(with_mean = False)),\n",
    "                    ('classify', LinearSVC(max_iter=10000, tol=0.1, dual = False))])\n",
    "pipe_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8838109992254067"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = pd.Series('''The rest are either tasteless or rehashed from IMOM.If this were the first movie of the series then I'd probably be easier on it. But the series started on a note of dry wit and then plummeted down to a level of gross out humour''')\n",
    "pipe_svm.predict(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
