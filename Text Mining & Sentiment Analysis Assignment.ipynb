{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import glob\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading file\n",
    "art1 = \"\"\"Your Stemming letter letter 100 of the 7th was received night before last. I very\n",
    "cheerfully send you the twenty dollars, which sum you say is\n",
    "necessary to save your land from sale. It is singular that you\n",
    "should have forgotten a judgment against you; and it is more\n",
    "singular that the plaintiff should have let you forget it so long,\n",
    "particularly as I suppose you have always had property enough to\n",
    "satisfy a judgment of that amount. Before you pay it, it would be\n",
    "well to be sure you have not paid it; or, at least, that you can\n",
    "not prove you have paid it. Give my love to Mother, and all the\n",
    "connections.\"\"\"\n",
    "data_pos = pd.read_csv(\"/Volumes/DATA/aclImdb/pos.csv\", sep=';', error_bad_lines=False)\n",
    "data_pos.columns = [\"target\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Task 2\n",
    "\n",
    "#Split in sentences\n",
    "\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#print ('\\n-----\\n'.join(tokenizer.tokenize(a)))\n",
    "\n",
    "\n",
    "#Remove stopwords and punctuations and stemming\n",
    "def remove_stop_words(text):\n",
    "    stopwords_en = stopwords.words(\"english\")\n",
    "    stopwords_en = stopwords_en + list(string.punctuation)\n",
    "    snow_stemmer = SnowballStemmer(language='english')\n",
    "    cleaned_tokens = []\n",
    "    for w in nltk.word_tokenize(text):\n",
    "        w = snow_stemmer.stem(w.lower())\n",
    "        if w not in stopwords_en:\n",
    "            cleaned_tokens.append(w)\n",
    "    cleaned_text = \" \".join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def split_sentence(article_text):\n",
    "    sentences = nltk.sent_tokenize(article_text)\n",
    "    return sentences\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def make_bigram(corpus):\n",
    "    tokens = tokenize(corpus)\n",
    "    bigram = nltk.ngrams(tokens, n = 2)\n",
    "    return bigram\n",
    "\n",
    "def generate_bow(document):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(document)\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(X.toarray())\n",
    "\n",
    "def tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform([art1])\n",
    "    tfidf = pd.DataFrame(X.toarray().transpose(), index=vectorizer.get_feature_names())\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def _init_(self):\n",
    "        return\n",
    "    \n",
    "    #def _init_(self):\n",
    "        #return\n",
    "\n",
    "    def fit( self, X, y = None ):\n",
    "        return self\n",
    "\n",
    "    def clean (self, x):\n",
    "        cleaned_text = remove_stop_words(x)\n",
    "        tokens = tokenize(cleaned_text)\n",
    "        return tokens\n",
    "\n",
    "    def transform( self, X, y = None ):\n",
    "        return self.clean(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stem',\n",
       " 'letter',\n",
       " 'letter',\n",
       " '100',\n",
       " '7th',\n",
       " 'receiv',\n",
       " 'night',\n",
       " 'befor',\n",
       " 'last',\n",
       " 'veri',\n",
       " 'cheer',\n",
       " 'send',\n",
       " 'twenti',\n",
       " 'dollar',\n",
       " 'sum',\n",
       " 'say',\n",
       " 'necessari',\n",
       " 'save',\n",
       " 'land',\n",
       " 'sale',\n",
       " 'singular',\n",
       " 'forgotten',\n",
       " 'judgment',\n",
       " 'singular',\n",
       " 'plaintiff',\n",
       " 'let',\n",
       " 'forget',\n",
       " 'long',\n",
       " 'particular',\n",
       " 'suppos',\n",
       " 'alway',\n",
       " 'properti',\n",
       " 'enough',\n",
       " 'satisfi',\n",
       " 'judgment',\n",
       " 'amount',\n",
       " 'befor',\n",
       " 'pay',\n",
       " 'would',\n",
       " 'well',\n",
       " 'sure',\n",
       " 'paid',\n",
       " 'least',\n",
       " 'prove',\n",
       " 'paid',\n",
       " 'give',\n",
       " 'love',\n",
       " 'mother',\n",
       " 'connect']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipel_task1 = Pipeline([('clean', CleanText())])\n",
    "pipel_task1.transform(art1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other pipeline for Part B\n",
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def _init_(self, lang = 'english'):\n",
    "        self.lang = lang\n",
    "        self.stemmer = SnowballStemmer(self.lang)\n",
    "\n",
    "    def fit( self, X, y = None ):\n",
    "        return self\n",
    "\n",
    "    def clean (self, x):\n",
    "        cleaned_text = remove_stop_words(x)\n",
    "        tokens = tokenize(cleaned_text)\n",
    "        return tokens\n",
    "\n",
    "    def transform( self, X, y = None ):\n",
    "        return self.clean(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
